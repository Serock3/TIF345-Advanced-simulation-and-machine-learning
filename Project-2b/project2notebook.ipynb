{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'icet'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\sebastian\\Documents\\GitHub\\TIF345-Advanced-simulation-and-machine-learning\\project2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file://c:\\Users\\sebastian\\Documents\\GitHub\\TIF345-Advanced-simulation-and-machine-learning\\project2.py?line=2'>3</a>\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      <a href='file://c:\\Users\\sebastian\\Documents\\GitHub\\TIF345-Advanced-simulation-and-machine-learning\\project2.py?line=3'>4</a>\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> <a href='file://c:\\Users\\sebastian\\Documents\\GitHub\\TIF345-Advanced-simulation-and-machine-learning\\project2.py?line=4'>5</a>\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0micet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClusterSpace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      <a href='file://c:\\Users\\sebastian\\Documents\\GitHub\\TIF345-Advanced-simulation-and-machine-learning\\project2.py?line=5'>6</a>\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      <a href='file://c:\\Users\\sebastian\\Documents\\GitHub\\TIF345-Advanced-simulation-and-machine-learning\\project2.py?line=6'>7</a>\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'icet'"]}],"source":["\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import KFold\n","from sklearn import linear_model\n","from icet import ClusterSpace\n","from ase.visualize import view\n","from ase.db import connect\n","import ase\n","import icet\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","from matplotlib.gridspec import GridSpec\n","import emcee\n","import corner\n","from scipy.stats import gamma, invgamma, t, norm, norminvgauss, mode\n","import seaborn as sns\n","sns.set_context(\"paper\", font_scale=1.5)\n","sns.set_style(\"darkgrid\")\n","sns.set_palette(\"deep\")\n","sns.set(font='sans-serif')\n","%matplotlib inline\n","plt.rcParams['figure.dpi'] = 140\n","\n","np.random.seed(123)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Project 2a: Alloy cluster expansions\n"," ### *Sebastian Holmin, Erik Andersson, 2020*\n"," # Task 1: Collect and plot the data (0.5p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","db = connect('structures/reference_data.db')\n","for i, row in enumerate(db.select()):\n","    atoms = row.toatoms()\n","    E_mix = row.mixing_energy\n","    print(i, row.symbols, E_mix)\n","    if(i == 3):\n","        view(atoms)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["E_mix_list = []\n","Pd_desity_list = []\n","for i, row in enumerate(db.select()):\n","    atoms = row.toatoms()\n","    atomic_numbers = atoms.get_atomic_numbers()\n","    E_mix = row.mixing_energy\n","    Pd_desity_list.append(sum(atomic_numbers == 46)/len(atomic_numbers))\n","    E_mix_list.append(E_mix)\n","plt.scatter(Pd_desity_list, E_mix_list, 4, 'black')\n","plt.xlabel('Pd concentration [a.u.]')\n","plt.ylabel('Energy per atom [meV]')\n","\n","plt.savefig('energy_vs_Pd_cons.pdf')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["atomic_numbers = atoms.get_atomic_numbers()\n","print(sum(atomic_numbers == 46)/len(atomic_numbers))\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Task 2: Cutoff selection for a pair cluster-expansion  (5p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","x = []\n","\n","# setup CS\n","cutoffs = [8]\n","prim = db.get(1).toatoms()\n","cs = ClusterSpace(prim, cutoffs=cutoffs, chemical_symbols=['Ag', 'Pd'])\n","\n","for i, row in enumerate(db.select()):\n","    # get cluster-vector for a given atoms object\n","    atoms = row.toatoms()\n","    x.append(cs.get_cluster_vector(atoms))\n","\n","x = np.array(x)\n","print(x.shape)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def run_OLS_fit(A, y):\n","    ols = linear_model.LinearRegression(fit_intercept=False)\n","    ols.fit(A, y)\n","    return ols.coef_\n","\n","\n","def compute_mse(A, y, parameters):\n","    y_predicted = np.dot(A, parameters)\n","    dy = y - y_predicted\n","    mse = np.mean(dy**2)\n","    return mse\n","\n","\n","def compute_rmse(A, y, parameters):\n","    return np.sqrt(compute_mse(A, y, parameters))\n","\n","\n","def get_aic_bic(A, y, parameters):\n","\n","    n_samples = len(y)\n","    n_parameters = len(parameters)\n","    mse = compute_mse(A, y, parameters)\n","\n","    aic = n_samples * np.log(mse) + 2 * n_parameters\n","    bic = n_samples * np.log(mse) + n_parameters * np.log(n_samples)\n","\n","    return -aic, -bic\n","\n","\n","def run_cv(A, y):\n","    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n","    rmse_train = []\n","    rmse_test = []\n","    for train_inds, test_inds in cv.split(A):\n","        A_train = A[train_inds]\n","        y_train = y[train_inds]\n","        A_test = A[test_inds]\n","        y_test = y[test_inds]\n","\n","        parameters = run_OLS_fit(A_train, y_train)\n","        rmse_train.append(compute_rmse(A_train, y_train, parameters))\n","        rmse_test.append(compute_rmse(A_test, y_test, parameters))\n","\n","    data = dict()\n","    data['rmse_train'] = np.mean(rmse_train)\n","    data['rmse_train_std'] = np.std(rmse_train)\n","    data['rmse_validation'] = np.mean(rmse_test)\n","    data['rmse_validation_std'] = np.std(rmse_test)\n","    return data\n","\n","\n","def full_analysis(A, y):\n","\n","    # run cv\n","    cv_data = run_cv(A, y)\n","\n","    # final fit\n","    parameters = run_OLS_fit(A, y)\n","    print(np.std(parameters))\n","    # compute AIC/BIC\n","    aic, bic = get_aic_bic(A, y, parameters)\n","\n","    # finalize data\n","    data = dict(aic=aic, bic=bic)\n","    data.update(cv_data)\n","    return data\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# generate data\n","np.random.seed(42)\n","cutoffs = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]]\n","cutoffs = [[x] for x in range(1, 20)]\n","data_list = []\n","\n","aic_list = []\n","bic_list = []\n","rmse_train_list = []\n","rmse_valid_list = []\n","std_train_list = []\n","std_valid_list = []\n","mse_list = []\n","\n","for cutoff in cutoffs:\n","    print('Cutoff= ', cutoff[0])\n","    x = []\n","\n","    # setup CS\n","    # cutoffs = [8]\n","    prim = db.get(1).toatoms()\n","    cs = ClusterSpace(prim, cutoffs=cutoff, chemical_symbols=['Ag', 'Pd'])\n","\n","    for i, row in enumerate(db.select()):\n","        # get cluster-vector for a given atoms object\n","        atoms = row.toatoms()\n","        x.append(cs.get_cluster_vector(atoms))\n","\n","    x = np.array(x)\n","    if cutoff[0] == 6:\n","        print('NUM PARAMETERS AT 6Å', x.shape[1])\n","    # N, M = 200, 100\n","    A = x\n","    # parameters_true = np.random.normal(0, 1, (M, ))\n","    # noise = np.random.normal(0, 0.05, (N, ))\n","    y = np.array(E_mix_list)\n","\n","    # OLS fit\n","    data = full_analysis(A, y)\n","    data_list.append(data)\n","    # for key, val in data.items():\n","    #     print(f'{key:20} : {val:11.5f}')\n","    # print('-----------------------\\n')\n","\n","    aic_list.append(data.get('aic'))\n","    bic_list.append(data.get('bic'))\n","    rmse_train_list.append(data.get('rmse_train'))\n","    rmse_valid_list.append(data.get('rmse_validation'))\n","    std_train_list.append(data.get('rmse_train_std'))\n","    std_valid_list.append(data.get('rmse_validation_std'))\n","\n","    parameters = run_OLS_fit(A, y)\n","    mse = compute_mse(A, y, parameters)\n","    mse_list.append(mse)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","fig_IC = plt.figure(figsize=(6.4*0.8, 4.8*0.8))\n","plt.plot(cutoffs, aic_list, label='AIC')\n","plt.plot(cutoffs, bic_list, label='BIC')\n","\n","\n","plt.xlabel('cutoff [Å]')\n","plt.ylabel('IC')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('cutoff_IC.pdf')\n","\n","fig_rmse = plt.figure(figsize=(6.4*0.8, 4.8*0.8))\n","plt.plot(cutoffs, rmse_train_list, label='Training')\n","plt.plot(cutoffs, rmse_valid_list, label='Validation')\n","\n","plt.xlabel('cutoff [Å]')\n","plt.ylabel('RMSE [meV]')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('cutoff_RMSE.pdf')\n","\n","fig_std = plt.figure()\n","plt.plot(cutoffs, std_train_list, label='Training')\n","plt.plot(cutoffs, std_valid_list, label='Validation')\n","\n","plt.xlabel('cutoff [Å]')\n","plt.ylabel('STD [meV]')\n","plt.legend()\n","\n","fig_mse = plt.figure('MSE')\n","plt.plot(cutoffs, mse_list)"]},{"cell_type":"markdown","metadata":{},"source":[" Task 3:  Feature selection  (5p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaler = StandardScaler()\n","scaler.fit(x)\n","print(np.std(x, axis=0))\n","x_stand = scaler.transform(x)\n","print(np.std(x_stand, axis=0))\n","print(np.mean(x_stand, axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def run_cv_Lasso(A, y, alpha):\n","    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n","    rmse_train = []\n","    rmse_test = []\n","\n","    lasso = linear_model.Lasso(alpha=alpha, fit_intercept=False)\n","\n","    for train_inds, test_inds in cv.split(A):\n","        A_train = A[train_inds]\n","        y_train = y[train_inds]\n","        A_test = A[test_inds]\n","        y_test = y[test_inds]\n","\n","        lasso.fit(A_train, y_train)\n","        parameters = lasso.coef_\n","\n","        rmse_train.append(compute_rmse(A_train, y_train, parameters))\n","        rmse_test.append(compute_rmse(A_test, y_test, parameters))\n","\n","    data = dict()\n","    data['rmse_train'] = np.mean(rmse_train)\n","    data['rmse_train_std'] = np.std(rmse_train)\n","    data['rmse_validation'] = np.mean(rmse_test)\n","    data['rmse_validation_std'] = np.std(rmse_test)\n","    return data\n","\n","\n","def get_aic_bic_sparse(A, y, parameters):\n","\n","    n_samples = len(y)\n","    n_parameters = sum(parameters != 0)\n","    mse = compute_mse(A, y, parameters)\n","\n","    aic = n_samples * np.log(mse) + 2 * n_parameters\n","    bic = n_samples * np.log(mse) + n_parameters * np.log(n_samples)\n","\n","    return -aic, -bic\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["alpha_list = np.linspace(0.2, 10, 100)\n","\n","aic_list = []\n","bic_list = []\n","rmse_train_list = []\n","rmse_valid_list = []\n","std_train_list = []\n","std_valid_list = []\n","mse_list = []\n","nbr_params_list = []\n","\n","y_stand = y\n","# y_stand=(y-np.mean(y))/np.std(y)\n","# A_stand=(x-np.mean(y))/np.std(y)\n","\n","A = x_stand\n","\n","for alpha in alpha_list:\n","\n","    # print(sum(params!=0))\n","\n","    cv_data = run_cv_Lasso(A, y_stand, alpha)\n","\n","    lasso = linear_model.Lasso(alpha=alpha, fit_intercept=False)\n","    lasso.fit(A, y_stand)\n","    params = lasso.coef_\n","    aic, bic = get_aic_bic_sparse(A, y_stand, params)\n","\n","    nbr_params_list.append(sum(params != 0))\n","    print(np.std(scaler.transform(params.reshape(1, 62))))\n","    aic_list.append(aic)\n","    bic_list.append(bic)\n","    rmse_train_list.append(cv_data.get('rmse_train'))\n","    rmse_valid_list.append(cv_data.get('rmse_validation'))\n","    std_train_list.append(cv_data.get('rmse_train_std'))\n","    std_valid_list.append(cv_data.get('rmse_validation_std'))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig_rmse = plt.figure()\n","plt.plot(alpha_list, rmse_train_list, label='Training')\n","plt.plot(alpha_list, rmse_valid_list, label='Validation')\n","\n","plt.xlabel(r'$\\alpha$ [a.u.]')\n","plt.ylabel('RMSE')\n","plt.legend()\n","# plt.xlim(0.5,10)\n","# plt.ylim(35,45)\n","\n","fig_rmse_vs_params = plt.figure(figsize=(6.4*0.8, 4.8*0.8))\n","plt.scatter(nbr_params_list, rmse_train_list, 4, label='Training')\n","plt.scatter(nbr_params_list, rmse_valid_list, 4, label='Validation')\n","\n","plt.xlabel(r'Number of parameters [a.u.]')\n","plt.ylabel('RMSE [meV]')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('Lasso_RMSE.pdf')\n","\n","fig_IC = plt.figure(figsize=(6.4*0.8, 4.8*0.8))\n","plt.scatter(nbr_params_list, aic_list, 4, label='AIC')\n","plt.scatter(nbr_params_list, bic_list, 4, label='BIC')\n","\n","plt.xlabel(r'Number of parameters [a.u.]')\n","plt.ylabel('IC')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('Lasso_IC.pdf')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","lmb = 100\n","ardr = linear_model.ARDRegression(threshold_lambda=lmb, fit_intercept=False)\n","ardr.fit(x_stand, y)\n","params = ardr.coef_\n","print(sum(params != 0))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def run_cv_ARDR(A, y, threshold_lambda):\n","    cv = KFold(n_splits=10, shuffle=True, random_state=42)\n","    rmse_train = []\n","    rmse_test = []\n","\n","    ardr = linear_model.ARDRegression(\n","        threshold_lambda=threshold_lambda, fit_intercept=False)\n","\n","    for train_inds, test_inds in cv.split(A):\n","        A_train = A[train_inds]\n","        y_train = y[train_inds]\n","        A_test = A[test_inds]\n","        y_test = y[test_inds]\n","\n","        ardr.fit(A_train, y_train)\n","        parameters = ardr.coef_\n","\n","        rmse_train.append(compute_rmse(A_train, y_train, parameters))\n","        rmse_test.append(compute_rmse(A_test, y_test, parameters))\n","\n","    data = dict()\n","    data['rmse_train'] = np.mean(rmse_train)\n","    data['rmse_train_std'] = np.std(rmse_train)\n","    data['rmse_validation'] = np.mean(rmse_test)\n","    data['rmse_validation_std'] = np.std(rmse_test)\n","    return data\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# lambda_list = np.append(np.linspace( 0, 0.9,10),np.linspace(1,10000,50))\n","lambda_list = np.append(np.linspace(0, 0.5, 100), np.linspace(1, 200, 20))\n","aic_list = []\n","bic_list = []\n","rmse_train_list = []\n","rmse_valid_list = []\n","std_train_list = []\n","std_valid_list = []\n","mse_list = []\n","nbr_params_list = []\n","\n","y_stand = y\n","# y_stand=(y-np.mean(y))/np.std(y)\n","# A_stand=(x-np.mean(y))/np.std(y)\n","\n","A = x_stand\n","for threshold_lambda in lambda_list:\n","\n","    # print(sum(params!=0))\n","\n","    cv_data = run_cv_ARDR(A, y_stand, threshold_lambda)\n","    ardr = linear_model.ARDRegression(\n","        threshold_lambda=threshold_lambda, fit_intercept=False)\n","    ardr.fit(A, y_stand)\n","    params = ardr.coef_\n","    aic, bic = get_aic_bic_sparse(A, y_stand, params)\n","\n","    nbr_params_list.append(sum(params != 0))\n","    aic_list.append(aic)\n","    bic_list.append(bic)\n","    rmse_train_list.append(cv_data.get('rmse_train'))\n","    rmse_valid_list.append(cv_data.get('rmse_validation'))\n","    std_train_list.append(cv_data.get('rmse_train_std'))\n","    std_valid_list.append(cv_data.get('rmse_validation_std'))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig_rmse = plt.figure()\n","plt.plot(lambda_list, rmse_train_list, label='Training')\n","plt.plot(lambda_list, rmse_valid_list, label='Validation')\n","\n","plt.xlabel(r'$\\lambda$ [a.u.]')\n","plt.ylabel('RMSE')\n","plt.legend()\n","plt.xlim(0, 1)\n","# plt.ylim(35,45)\n","\n","fig_lambda_params = plt.figure()\n","plt.semilogx(lambda_list, nbr_params_list)\n","\n","plt.xlabel(r'$\\lambda$ [a.u.]')\n","plt.ylabel('Number of parameters [a.u.]')\n","# plt.xlim(0,1)\n","\n","fig_rmse_vs_params = plt.figure(figsize=(6.4*0.8, 4.8*0.8))\n","plt.scatter(nbr_params_list, rmse_train_list, 4, label='Training')\n","plt.scatter(nbr_params_list, rmse_valid_list, 4, label='Validation')\n","\n","plt.xlabel(r'Number of parameters [a.u.]')\n","plt.ylabel('RMSE [meV]')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('ARDR_RMSE.pdf')\n","\n","fig_IC = plt.figure(figsize=(6.4*0.8, 4.8*0.8))\n","plt.scatter(nbr_params_list, aic_list, 4, label='AIC')\n","plt.scatter(nbr_params_list, bic_list, 4, label='BIC')\n","\n","plt.xlabel(r'Number of parameters [a.u.]')\n","plt.ylabel('IC')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('ARDR_IC.pdf')\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Task 4: Bayesian Cluster expansion   (7p)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Get new cluster space with cutoffs\n","x = []\n","\n","# setup CS\n","cutoffs = [12.0, 6.0]\n","prim = db.get(1).toatoms()\n","cs = ClusterSpace(prim, cutoffs=cutoffs, chemical_symbols=['Ag', 'Pd'])\n","\n","for i, row in enumerate(db.select()):\n","    # get cluster-vector for a given atoms object\n","    atoms = row.toatoms()\n","    x.append(cs.get_cluster_vector(atoms))\n","\n","x = np.array(x)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define priors and likelihood\n","\n","\n","def mean_mode_2_IG_alpha_beta(mean, mode):\n","    alpha = (mode + mean)/(mean-mode)\n","    beta = (2 * mode * mean)/(mean-mode)\n","    return alpha, beta\n","\n","\n","a0_sig, b0_sig = mean_mode_2_IG_alpha_beta(100, 1)\n","a0_alpha, b0_alpha = mean_mode_2_IG_alpha_beta(2500, 25)\n","\n","\n","def log_prior(j, sigma2, alpha2, nP):\n","    return -0.5*nP*np.log(alpha2)-0.5*np.sum(j**2)/alpha2 + invgamma.logpdf(sigma2, a=a0_sig, scale=b0_sig) + invgamma.logpdf(alpha2, a=a0_alpha, scale=b0_alpha)\n","\n","\n","def log_likelihood(model, sigma2, data):\n","    return -np.sum((model-data)**2)/(2*sigma2)-0.5*len(data)*np.log(sigma2)\n","\n","\n","def log_posterior(params, A, data, nP):\n","    j = params[:nP]\n","    sigma2 = params[nP]\n","    alpha2 = params[nP+1]\n","\n","    model = np.matmul(A, j)\n","\n","    lp = log_prior(j, sigma2, alpha2, nP)\n","    if not np.isfinite(lp):\n","        return -np.inf\n","\n","    return log_likelihood(model, sigma2, data)+lp\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ndim, nwalkers = x.shape[1]+2, 100\n","# start_pos = [70,0,0.1] + [1e-2,1e-2,1e-5]*np.random.randn(nwalkers, ndim)\n","start_pos = [0]*x.shape[1]+[10]*np.random.randn(nwalkers, ndim-2)\n","start_pos = np.append(start_pos, [7, 30]+[1e-1]\n","                      * np.random.randn(nwalkers, 2), axis=1)\n","\n","steps = 5000\n","\n","\n","sampler = emcee.EnsembleSampler(\n","    nwalkers, ndim, log_posterior, args=(x, y, ndim-2))\n","sampler.run_mcmc(start_pos, steps, progress=True)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def simple_mcmc_analysis(sampler, par, label, burn_in, chain_from_file=False):\n","\n","    if not chain_from_file:\n","        print(\n","            f'Mean acceptance fraction: {np.mean(sampler.acceptance_fraction):0.3f}')\n","    # discard the first 'burn_in' samples\n","\n","    # thinning means that you only keep every nth sample. E.g. thinning=10 -> keep every 10th sample.\n","    # This can be useful for reducing long autocorrelation lenghts in a chain. However, thinning is expensive.\n","    # A thinned chain must be run E.g. 10x longer to reach the desired number of samples.\n","    # One can argue that thinning is not an advantageous strategy. So keep thinning = 1\n","    thinning = 1\n","    flat_mcmc_samples = sampler.get_chain(\n","        discard=burn_in, thin=thinning, flat=True)\n","    print(f'Discarding {nwalkers*burn_in} steps as burn-in')\n","    print(f'Chain length:{len(flat_mcmc_samples)}')\n","\n","    fig1 = plt.figure()\n","    plt.plot(flat_mcmc_samples[:, par], color='gray', alpha=0.7)\n","    plt.xlabel('Sample')\n","    plt.ylabel(label)\n","    plt.xlim(0, len(flat_mcmc_samples))\n","\n","    return flat_mcmc_samples\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["burn_in = 3000\n","\n","flat_mcmc_samples = simple_mcmc_analysis(\n","    sampler, par=0, label=f'$H_0$', burn_in=burn_in)\n","\n","fig = corner.corner(flat_mcmc_samples[:, 33:35], show_titles=True)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db_gs = connect('structures/ground_state_candidates.db')\n","\n","x_gs = []\n","\n","for i, row in enumerate(db_gs.select()):\n","    atoms = row.toatoms()\n","    x_gs.append(cs.get_cluster_vector(atoms))\n","\n","x_gs = np.array(x_gs)\n","\n","parameters = run_OLS_fit(x, y)\n","E_cand = np.matmul(x_gs, parameters)\n","\n","gs_index = np.argmin(E_cand)\n","print(gs_index)\n","for i, row in enumerate(db_gs.select()):\n","    if i == gs_index:\n","        view(row.toatoms())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Calculate lowest energy candidate for each sample,\n","# save lowest energy for each sample.\n","\n","E_gs = []\n","gs_freq = np.zeros(x_gs.shape[0])\n","\n","for i in range(len(flat_mcmc_samples[:, 0])):\n","    E = np.matmul(x_gs, flat_mcmc_samples[i, :33])\n","    index = np.argmin(E)\n","    E_gs.append(E[index])\n","    gs_freq[index] += 1\n","\n","print(gs_freq/np.sum(gs_freq))\n","plt.hist(E_gs, bins=100)\n","\n","gs_index_bayes = np.argmax(gs_freq)\n","\n","for i, row in enumerate(db_gs.select()):\n","    if i == gs_index_bayes:\n","        view(row.toatoms())\n","print(gs_index_bayes)\n","\n","# Frequency of being the lowest is converted to probability of the same.\n","\n","# Plot the distribution of lowest energies.\n",""]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.errorbar(range(33), np.mean(flat_mcmc_samples_copy[:, :33], axis=0), yerr=np.std(\n","    flat_mcmc_samples_copy[:, :33], axis=0), fmt='.k', capsize=1.5, linewidth=0.7, markersize=4)\n","plt.scatter(range(33), parameters)\n",""]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}